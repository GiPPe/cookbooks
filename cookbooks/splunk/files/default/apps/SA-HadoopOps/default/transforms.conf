# Copyright 2011 Splunk Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

[rack_host_mapping]
filename = rack_host_mapping.csv

[hadoop_jobs]
filename = hadoop_jobs.csv

[hadoop_host2cluster]
filename = hadoop_host2cluster.csv

[hadoop_host2hdfs]
filename = hadoop_host2hdfs.csv

[hadoop_host2mapred]
filename = hadoop_host2mapred.csv

[hadoop_components]
filename = hadoop_components.csv

[hadoop_news_feed]
filename = hadoop_news_feed.csv

[hadoop_host2rack]
filename = hadoop_host2rack.csv

[hadoop_host2maxcpu]
filename = hadoop_host2maxcpu.csv

[hadoop_severity]
REGEX=\d+:\d+:\d+\,\d+\s+(\S+)
FORMAT=severity::$1

[hadoop_metrics_namespace]
SOURCE_KEY=source
REGEX=\/([^\-\/]+)\-metrics.out
FORMAT=namespace::$1

[hadoop_metrics_context]
REGEX=^\S+\s+([^\.]+).([^\:]+)
FORMAT=context::$1 sub_context::$2

[hadoop_conf_xml]
REGEX=(?ms)<property>.*?<name>([^\<]+)</name>.*?<value>([^\<]+)</value>
FORMAT=$1::$2
REPEAT_MATCH=True

[hadoop_attempt]
REGEX=attempt\_((([^\_]+)\_([^\_]+))\_([^\_]+)\_([^\_]+)\_(\d+))
FORMAT=attempt_id::$1 job_id::$2 daemon_start_time::$3 job_number::$4 task_type::$5 task_number::$6 attempt_number::$7
REPEAT_MATCH=True

[hadoop_task]
REGEX=task\_((([^\_]+)\_([^\_]+))\_([^\_]+)\_([^\s\'\,]+))
FORMAT=task_id::$1 job_id::$2 daemon_start_time::$3 job_number::$4 task_type::$5 task_number::$6 
REPEAT_MATCH=True

[hadoop_file_cache_size]
REGEX=FILE_CACHE_SIZE\sfor\smapOutputServlet\sset\sto\s\:\s(\S+)
FORMAT=file_cache_size::$1

[hadoop_metrics_registered]
REGEX=MetricsSourceAdapter\:\sMBean\sfor\ssource\s(\S+)\sregistered
FORMAT=mbean::$1

[hadoop_index_cache]
REGEX=IndexCache\screated\swith\smax\smemory\s\=\s(\S+)
FORMAT=index_cache_max_mem::$1

[hadoop_task_mem_mgr]
REGEX=totalMemoryAllottedForTasks\sis\s([^\.\s]+)
FORMAT=total_task_mem::$1

[hadoop_tracker]
REGEX=\stracker\_(\S+)
FORMAT=tracker::$1
REPEAT_MATCH=True
MV_ADD=True

[hadoop_mem_calc_plugin]
REGEX=Using\sMemoryCalculatorPlugin\s\:\s(\S+)
FORMAT=mem_calc_plugin::$1

[hadoop_res_calc_plugin]
REGEX=Using\sResourceCalculatorPlugin\s\:\s(\S+)
FORMAT=resource_calc_plugin::$1

[hadoop_ipc_server_handler]
REGEX=IPC\sServer\shandler\s(\S+)\son\s([^\:])\:\s(\S+)
FORMAT=ipc_server_handler::$1 ipc_server_handler_port::$2 action::$3

[hadoop_ipc_server_listener]
REGEX=IPC\sServer\slistener\son\s([^\:]+)\:\s(\S+)
FORMAT=ipc_server_listener_port::$1 action::$2

[hadoop_task_tracker_owner]
REGEX=(Starting)\stasktracker\swith\sowner\sas\s(\S+)
FORMAT=action::$1 owner::$2

[hadoop_task_log_truncator]
REGEX=Initializing\slogs\'\struncater\swith\smapRetainSize=(\S+)\sand\sreduceRetainSize\=(\S+)
FORMAT=map_retain_size::$1 reduce_retain_size::$2

[hadoop_metrics_snapshot]
REGEX=Scheduled\ssnapshot\speriod\sat\s(\S+)\ssecond
FORMAT=snapshot_period::$1

[hadoop_local_mapred_dir]
REGEX=mapred\slocal\sdirectories\sare\:\s(\S+)
FORMAT=local_mapred_dir::$1

[hadoop_client_resending]
REGEX=Resending\s\'([^\']+)\'\sto\s\'([^\']+)\'\swith\sresponseId\s\'([^\'\s]+)
FORMAT=action::$1 server::$2 response_id::$3

[hadoop_client_retry]
REGEX=Retrying\sconnect\sto\sserver\:\s(\S+)\.\sAlready\stried\s(\S+)\stime
FORMAT=server::$1 retry_count::$2

[hadoop_client_mismatch]
REGEX=version\smismatch\.\s\(client\s\=\s([^\,]+)\,\sserver\s\=\s([^\)]+)
FORMAT=client_version::$1 server_version::$2

[hadoop_add_path]
REGEX=\sfor\suser\-log\sdeletion\swith\sretainTimeStamp\:(\S+)
FORMAT=job_retain_time::$1

[hadoop_job]
REGEX=job\_(([^\_]+)\_(\d+))
FORMAT=job_id::$1 daemon_start_time::$2 job_number::$3
REPEAT_MATCH=True

[hadoop_job_action_received]
REGEX=Received\s\'([^\']+)\'\sfor\sjob\:
FORMAT=action::$1 

[hadoop_job_name]
REGEX=JOBNAME\=\"([^\"]+)\"\s
FORMAT=job_name::$1

[hadoop_task_action_received]
REGEX=Received\s(\S+)\sfor\stask\:
FORMAT=action::$1 

[hadoop_free_slots]
REGEX=current\sfree\sslots\s\:\s(\d+)
FORMAT=free_slots::$1

[hadoop_output_size]
REGEX=reported\soutput\ssize\sfor\s\S+\s+was\s(\S+)
FORMAT=output_size::$1

[hadoop_perc_1]
REGEX=\s(\S+)\%\s(cleanup)
FORMAT=perc_complete::$1 action::$2

[hadoop_perc_2]
REGEX=\s(\S+)\%\s(\S+)\s\S+\s(\S+)
FORMAT=perc_complete::$1 action::$2 sub_action::$3

[hadoop_perc_3]
REGEX=\s(\S+)\%\shdfs
FORMAT=perc_complete::$1

[hadoop_copy]
REGEX=copy\s\((\d+)\sof\s(\d+)\sat\s(\S+)
FORMAT=bytes_xfered::$1 bytes_total::$2 xfer_rate::$3

[hadoop_jvm]
REGEX=jvm\_((([^\_]+)\_([^\_]+))\_([^\_]+)\_(\S+))
FORMAT=jvm_id::$1 job_id::$2 daemon_start_time::$3 job_number::$4 task_type::$5 jvm_hash_id::$6

[hadoop_jvm_exit]
REGEX=exited\swith\sexit\scode\s(\d+)\.\sNumber\sof\stasks\sit\sran\:\s(\d+)
FORMAT=exit_code::$1 tasks_run::$2

[hadoop_slots_needed]
REGEX=which\sneeds\s(\d+)\sslots
FORMAT=needed_slots::$1

[hadoop_task_state]
REGEX=task(?:\'s)?\sstate\:(\S+)
FORMAT=task_state::$1

[hadoop_kill_pg]
REGEX=Killing\sprocess\s(\S+)\swith\ssignal\s([^\.]+)\.\sExit\scode\s(\S+)
FORMAT=process_group::$1 signal::$2 exit_code::$3

[hadoop_cmd_write]
REGEX=Writing\scommands\sto\s((.*)\/job\_.*\/([^\/]+))
FORMAT=hdfs_local_path::$1 hdfs_local_base_path::$2 command::$3 

[hadoop_client_trace]
REGEX=clienttrace\:\ssrc\:\s([^\:]+)\:(\d+)\,\sdest\:\s([^\:]+)\:(\d+)\,\sbytes\:\s(\d+)\,\sop\:\s([^\,]+)\,\scliID\:\s[^\,]+\,\sduration\:\s(\d+)
FORMAT=src_ip::$1 src_port::$2 dest_ip::$3 dest_port::$4 bytes_sent::$5 operation::$6 duration::$7

[hadoop_hdfs_1]
REGEX=hdfs:\/\/(([^\:]+)\:([^\/]+)\/(.*)\/([^\/\:]+)\:([^\+]+)\+(\S+))
FORMAT=hdfs_remote_path::$1 hdfs_server::$2 hdfs_port::$3 hdfs_server_path::$4 hdfs_remote_file::$5 offset::$6 chunk_size::$7
 
[hadoop_hdfs_2]
REGEX=problem\scleaning\ssystem\sdirectory\:\shdfs:\/\/(([^\:]+)\:([^\/]+)\/(\S+))
FORMAT=hdfs_remote_path::$1 hdfs_server::$2 hdfs_port::$3 hdfs_server_path::$4

[hadoop_hdfs_3]
REGEX=Writing\sto\sfile\shdfs:\/\/(([^\:]+)\:([^\/]+)\/(\S+))
FORMAT=hdfs_remote_path::$1 hdfs_server::$2 hdfs_port::$3 hdfs_server_path::$4

[hadoop_native_user]
REGEX=Got\sUserName\s(\S+)\sfor\sUID\s(\S+)\sfrom\sthe\snative
FORMAT=user_name::$1 user::$1 uid::$2

[hadoop_no_such_user]
REGEX=\sid\:\s([^\:]+)\:\sNo\ssuch\suser
FORMAT=user_name::$1

[hadoop_cache_create]
REGEX=TrackerDistributedCacheManager\:\sCreating\s(\S+)\sin\s(\S+)
FORMAT=hdfs_local_file::$1 hdfs_local_path::$2

[hadoop_cache_extract]
REGEX=TrackerDistributedCacheManager\:\sExtracting\s(\S+)\sto\s(\S+)
FORMAT=hdfs_local_path::$1 hdfs_local_cache_path::$2

[hadoop_cache_done]
REGEX=TrackerDistributedCacheManager\:\sCached\s(\S+)\sas\s(\S+)
FORMAT=hdfs_remote_path::$1 hdfs_local_path::$2

[hadoop_user_init]
REGEX=Initializing\suser\s(\S+)
FORMAT=user_name::$1

[hadoop_user_name]
REGEX=\sUSER\=\"([^\"]+)\"
FORMAT=user_name::$1

[hadoop_user_on_retire]
REGEX=Retired\sjob\swith\sid\:\s\S+\sof\suser\s\'([^\']+)
FORMAT=user_name::$1

[hadoop_metrics_duplicate]
REGEX=MetricsSystemImpl\:\sSource\sname\s(\S+)\salready\sexists\!
FORMAT=metrics_source::$1

[hadoop_server_port]
REGEX=Stopping\sserver\son\s(\d+)
FORMAT=server_port::$1

[hadoop_binding]
REGEX=Problem\sbinding\sto\s([^\/]+)\/([^\:]+)\:(\S+)
FORMAT=server_name::$1 server_ip::$2 server_port::$3

[hadoop_lost_tracker]
REGEX=Lost\stracker\s\'tracker\_([^\']+)
FORMAT=tracker::$1

[hadoop_file_replication]
REGEX=File\s(\S+)\scould\sonly\sbe\replicated\sto\s(\d+)\snodes\,\sinstead\sof\s(\d+)
FORMAT=local_file::$1 actual_nodes::$2 expected_nodes::$3

[hadoop_tracker_host]
REGEX=Adding\stracker\stracker\_\S+\sto\shost\s(\S+)
FORMAT=remote_host::$1

[hadoop_node]
REGEX=\snode\:\s?(\/([^\/]+)\/(([^\.]+)\S+))
FORMAT=node_name::$1 rack_name::$2 remote_host_fqdn::$3 remote_host::$4 

[hadoop_decom_node]
REGEX=JobTracker\:\sDecommissioning\s(\d+)\snodes
FORMAT=decommissioned_nodes::$1

[hadoop_done_dir]
REGEX=JobHistory\:\sCreating\sDONE\s(?:folder|subfolder)\sat\sfile\:(\S+)
FORMAT=done_dir::$1

[hadoop_conf_del]
REGEX=JobHistory\:\sDeleting\slocalized\sjob\sconf\sat\s((\S+)\/([^\/]+))
FORMAT=full_path::$1 base_path::$2 local_file::$3

[hadoop_moving_file]
REGEX=JobHistory\:\sMoving\sfile\:((\S+)\/([^\/\s]+))\sto\sfile\:(\S+)
FORMAT=full_path::$1 base_path::$2 local_file::$3 done_path::$4

[hadoop_done_subdirs]
REGEX=JobHistory\:\sexistingDoneSubdirs\sdoesn\'t\scontain\sfile\:([^\,]+)
FORMAT=done_path::$1

[hadoop_tracker]
REGEX=tracker\_([^\s\']+)
FORMAT=tracker::$1
REPEAT_MATCH=True

[hadoop_choose_rack]
REGEX=JobInProgress\:\sChoosing\s(rack\-\S+)
FORMAT=rack_name::$1

[hadoop_choose_data]
REGEX=JobInProgress\:\sChoosing\s(data-\S+)
FORMAT=data::$1

[hadoop_job_init]
REGEX=JobInProgress\:\sJob\s\S+\sinitialized\ssuccessfully\swith\s(\d+)\smap\stasks\sand\s(\d+)\sreduce
FORMAT=map_tasks::$1 reduce_tasks::$2

[hadoop_locality]
REGEX=LOCALITY_WAIT_FACTOR\=(\S+)
FORMAT=wait_factor::$1

[hadoop_size_splits]
REGEX=JobInProgress\:\sInput\ssize\sfor\sjob\s\S+\s\=\s(\d+)\.\sNumber\sof\ssplits\s\=\s(\d+)
FORMAT=input_size::$1 splits::$2

[hadoop_job_add]
REGEX=\sadded\ssuccessfully\sfor\suser\s\'([^\']+)\'\sto\squeue\s\'([^\']+)
FORMAT=user_name::$1 queue_name::$2

[hadoop_job_maps_reduces]
REGEX=\snMaps\=(\d+)\snReduces\=(\d+)\smax\=(\S+)
FORMAT=maps::$1 reduces::$2 max_ops::$3

[hadoop_task_failed_count]
REGEX=TaskInProgress\s\S+\shas\sfailed\s(\d+)\stimes
FORMAT=failure_count::$1

[hadoop_fetch_failure_count]
REGEX=JobInProgress\:\sFailed\sfetch\snotification\s\#(\d+)
FORMAT=failure_count::$1

[hadoop_flaky]
REGEX=JobInProgress\:\sTaskTracker\sat\s\'([^\']+)
FORMAT=remote_host::$1

[hadoop_large_response]
REGEX=ipc\.Server\:\sLarge\sresponse\ssize\s(\d+)\sfor\scall\s(\S+)\sfrom\s([^\:]+)\:(\S+)
FORMAT=bytes_received::$1 call::$2 remote_host::$3 remote_post::$4

[hadoop_dfs_source_file]
REGEX=Source\sfile\s\"([^\"]+)
FORMAT=local_file::$1

[hadoop_block]
REGEX=blk\_(([^\_]+)\_(\d+))
FORMAT=block_id::$1 block_offset::$2 block_gen::$3
REPEAT_MATCH=True
MV_ADD=True

[hadoop_block_report]
REGEX=BlockReport\sof\s(\d+)\sblocks\stook\s(\d+)\smsecs?\sto\sgenerate\sand\s(\d+)
FORMAT=total_blocks::$1 gen_time::$2 proc_time::$3

[hadoop_block_scan]
REGEX=Reconciled\sasynchronous\sblock\sscan\swith\sfilesystem\.\s(\d+)\sblocks\sconcurrently\sdeleted\sduring\sscan\,\s(\d+)\sblocks\sconcurrently\sadded\sduring\sscan\,\s(\d+)
FORMAT=blocks_deleted::$1 blocks_added::$2 blocks_ignored::$3

[hadoop_block_receiving]
REGEX=\ssrc\:\s\/([^\:]+)\:(\d+)\sdest\:\s\/([^\:]+)\:(\d+)
FORMAT=src_ip::$1 src_port::$2 dest_ip::$3 dest_port::$4

[hadoop_block_deleted]
REGEX=(?:Deleted|Deleting)\sblock\s\S+(?:\sat)?\sfile\s(\S+)
FORMAT=file_name::$1

[hadoop_block_scheduling]
REGEX=Scheduling\sblock\s\S+\sfile\s(\S+)\sfor\s(\S+)
FORMAT=file_name::$1 action::$2

[hadoop_dn_reg_1]
REGEX=DatanodeRegistration\(([^\,]+)\,\sstorageID\=([^\,]+)\,\sinfoPort\=(\d+)\,\sipcPort\=(\d+)
FORMAT=node_name::$1 storage_id::$2 info_port::$3 ipc_port::$4 

[hadoop_dn_reg_2]
REGEX=In\sDatanode.run\,\sdata\s?\=\sFSDataset\{dirpath\=\'([^\']+)
FORMAT=hdfs_local_path::$1

[hadoop_storage_id_assigned]
REGEX=New\sstorage\sid\s(\S+)\sis\sassigned\sto\sdata\-node\s(([^\:]+)\:(\d+))
FORMAT=storage_id::$1 node_name::$2 src_ip::$3 src_port::$4 

[hadoop_rpc_server]
REGEX=RPC\:\sServer\sat\s(([^\/]+)\/([^\:]+)\:(\d+))
FORMAT=rpc_server::$1 dest_host::$2 dest_ip::$3 dest_port::$4

[hadoop_incompatible_nsid]
REGEX=Incompatible\snamespaceIDs\sin\s([^\:]+)\:\snamenode\snamespaceID\s\=\s([^\;]+)\;\sdatanode\snamespaceID\s\=\s(\d+)
FORMAT=hdfs_local_path::$1 namenode_id::$2 datanode_id::$3

[hadoop_incompatible_build]
REGEX=Incompatible\sbuild\sversions\:\snamenode\sBV\s\=\s([^\;]+)\;\sdatanode\sBV\s\=\s(\d+)
FORMAT=namenode_build::$1 datanode_build::$2

[hadoop_of_size]
REGEX=\sof\ssize\s(\d+)$
FORMAT=block_size::$1

[hadoop_block_recover]
REGEX=targets\=\[([^\:]+)\:(\d+)
FORMAT=dest_ip::$1 dest_port::$2

[hadoop_old_new]
REGEX=\,\sdatanode=([^\:]+)\:(\d+)$
FORMAT=dest_ip::$1 dest_port::$2

[hadoop_block_from]
REGEX=\sfrom\s\/([^\:]+)\:(\d+)
FORMAT=src_ip::$1 src_port::$2

[hadoop_block_to]
REGEX=\sto\s\/([^\:]+)\:(\d+)
FORMAT=dest_ip::$2 dest_port::$2

[hadoop_process_report]
REGEX=processReport\:\sfrom\s([^\:]+)\:(\d+)\,\sblocks\:\s(\d+)\,\sprocessing\stime\:\s(\d+)
FORMAT=src_ip::$1 src_port::$2 blocks::$3 proc_time::$4

[hadoop_trans_sync]
REGEX=FSNamesystem\:\sNumber\sof\stransactions\:\s(\d+)\sTotal\stime\sfor\stransactions\(ms\)\:\s(\d+)\s?Number\sof\stransactions\sbatched\sin\sSyncs\:\s(\d+)\sNumber\sof\ssyncs\:\s(\d+)\sSyncTimes\(ms\)\:\s(\d+)
FORMAT=transactions::$1 transaction_time::$2 transactions_batched::$3 syncs::$4 sync_time::$5

[hadoop_complete_file]
REGEX=NameSystem\.completeFile\:\sfile\s(\S+)\sis\sclosed\sby\s(\S+)$
FORMAT=file_name::$1 holder::$2

[hadoop_remove_lease]
REGEX=StateChange\:\sRemoving\slease\son\s+file\s(\S+)\sfrom\sclient\s(\S+)
FORMAT=file_name::$1 holder::$2

[hadoop_add_stored_block]
REGEX=NameSystem\.addStoredBlock\:\sblockMap\supdated\:\s([^\:]+)\:(\d+)\sis\sadded\sto\s\S+\ssize\s(\d+)
FORMAT=dest_ip::$1 dest_port::$2 block_size::$3

[hadoop_ask_replicate]
REGEX=ask\s([^\:]+)\:(\d+)\sto\sreplicate\s\S+\sto\sdatanode\(s\)\s+([^\:]+)\:(\d+)$
FORMAT=src_ip::$1 src_port::$2 dest_ip::$3 dest_port::$4 

[hadoop_lease_removed]
REGEX=lease\sremoved\sfor\s(\S+)
FORMAT=file_name::$1

[hadoop_invalid_set]
REGEX=to\sinvalidSet\sof\s([^\:]+)\:(\d+)
FORMAT=dest_ip::$1 dest_port::$2

[hadoop_rename_failed]
REGEX=failed\sto\srename\s(\S+)\sto\s(\S+)
FORMAT=file_name::$1

[hadoop_block_sync]
REGEX=commitBlockSynchronization\(newblock\=\S+\sfile\=([^\s\,]+)\,\snewgenerationstamp\=(\d+)\,\snewlength\=(\d+)\,\snewtargets\=\[([^\:]+)\:(\d+)
FORMAT=file_name::$1 block_size::$3 dest_ip::$4 dest_port::$5

[hadoop_block_allocation]
REGEX=NameSystem\.allocateBlock\:\s\/([^\/]+)\/([^\/]+)\.
FORMAT=rack_name::$1 file_name::$2

[hadoop_as_user]
REGEX=PriviledgedActionException\sas\:(\S+)
FORMAT=user_name::$1

[hadoop_no_lease]
REGEX=LeaseExpiredException\:\sNo\slease\son\s(\S+).*Holder\s(\S+)
FORMAT=file_name::$1 holder::$2

[hadoop_ipc_version_mismatch]
REGEX=Incorrect\sheader\sor\sversion\smismatch\sfrom\s([^\:]+)\:(\d+)\sgot\sversion\s(\d+)\sexpected\sversion\s(\d+)
FORMAT=dest_ip::$1 dest_port::$2 client_version::$3 server_version::$4

[hadoop_queue_stats_1]
REGEX=QueueProcessingStatistics\:\sQueue\sflush\scompleted\s(\d+)\sblocks\sin\s(\d+)\smsec\sprocessing\stime\,\s(\d+)\smsec\sclock\stime\,\s(\d+)
FORMAT=blocks::$1 proc_time::$2 clock_time::$3 cycles::$4

[hadoop_queue_stats_2]
REGEX=QueueProcessingStatistics\:\sFirst\scycle\scompleted\s(\d+)\sblocks\sin\s(\d+)\smsec
FORMAT=blocks::$1 proc_time::$2

[hadoop_network_topology]
REGEX=Network\stopology\shas\s(\d+)\sracks\sand\s(\d+)
FORMAT=racks::$1 datanodes::$2

[hadoop_over_blocks]
REGEX=Number\sof\s+over\-replicated\sblocks\s\=\s(\d+)
FORMAT=over_replicated_blocks::$1

[hadoop_under_blocks]
REGEX=Number\sof\s+under\-replicated\sblocks\s\=\s(\d+)
FORMAT=under_replicated_blocks::$1

[hadoop_invalid_blocks]
REGEX=Number\sof\s+invalid\sblocks\s\=\s(\d+)
FORMAT=invalid_blocks::$1

[hadoop_total_blocks]
REGEX=Total\snumber\sof\sblocks\s\=\s(\d+)
FORMAT=total_blocks::$1

[hadoop_data_node_reg]
REGEX=node\sregistration\sfrom\s([^\:]+)\:(\d+)\sstorage\s(\S+)
FORMAT=src_ip::$1 src_port::$2 storage_id::$3

[hadoop_name_node_web_server]
REGEX=NameNode\:\sWeb\-server\sup\sat\:\s([^\:]+)\:(\d+)
FORMAT=src_ip::$1 src_port::$2

[hadoop_name_node_up]
REGEX=Namenode\sup\sat\:\s+([^\/]+)\/([^\:]+)\:(\d+)
FORMAT=name_node::$1 src_ip::$2 src_port::$3

[hadoop_fsimage]
REGEX=Finished\sloading\sFSImage\sin\s(\d+)
FORMAT=load_time::$1

[hadoop_name_cache]
REGEX=NameCache\:\sinitialized\with\s(\d+)\sentries\s(\d+)
FORMAT=name_cache_entries::$1 name_cache_lookups::$2

[hadoop_image_file]
REGEX=Storage\:\sImage\sfile\sof\ssize\s(\d+)\s(?:saved|loaded)\sin\s(\d+)
FORMAT=file_name::$1 proc_time::$2

[hadoop_edits_file]
REGEX=Edits\sfile\s(\S+)\sof\ssize\s(\d+)\sedits\s\#\s(\d+)\sloaded\sin\s(\d+)
FORMAT=file_name::$1 total_edits::$2 proc_time::$4

[hadoop_inconsistent_state]
REGEX=Directory\s(\S+)\sis\sin\san\sinconsistent\sstate
FORMAT=file_name::$1

[hadoop_directory_does_not_exist]
REGEX=Storage\:\sStorage\sdirectory\s(\S+)\s
FORMAT=file_name::$1

[hadoop_output_error]
REGEX=IPC\sServer\sResponder\,\scall\s([^(]+)\(([^\)]+)\)\sfrom\s([^\:]+)\:(\d+)
FORMAT=method_name::$1 method_args::$2 src_ip::$3 src_port::$4

[hadoop_replication_increasing]
REGEX=FSNamesystem\:\sIncreasing\sreplication\sfor\sfile\s(\S+)\.\sNew\sreplication\sis\s(\d+)
FORMAT=file_name::$1 replication_factor::$2

[hadoop_choose_excess]
REGEX=chooseExcessReplicates\:\s\(([^\:]+)\:(\d+)\,
FORMAT=dest_ip::$1 dest_port::$2

[hadoop_invalidate_block]
REGEX=invalidateBlock\:\s\S+\son\s([^\:]+)\:(\d+)
FORMAT=dest_ip::$1 dest_port::$2

[hadoop_corrupt_block]
REGEX=added\sas\corrupt\son\s([^\:]+)\:(\d+)\sby\s\/([\S+)
FORMAT=dest_ip::$1 dest_port::$2 src_ip::$3

[hadoop_mark_corrupt]
REGEX=Mark\snew\sreplica\s\S+\sfrom\s([^\:]+)\:(\d+)
FORMAT=src_ip::$1 src_port::$2

[hadoop_inconsistent_size]
REGEX=Inconsistent\ssize\sfor\sblock\s\S+\sreported\sfrom\s([^\:]+)\:(\d+)\scurrent\ssize\sis\s(\d+)\sreported\ssize\sis\s(\d+)
FORMAT=src_ip::$1 src_port::$2 current_size::$3 reported_size::$4

[hadoop_lost_heartbeat]
REGEX=heartbeatCheck\:\s+lost\sheartbeat\sfrom\s([^\:]+)\:(\d+)
FORMAT=src_ip::$1 src_port::$2

### Job history

[hadoop_total_maps]
REGEX=TOTAL\_MAPS\=\"(\d+)\"
FORMAT=job_total_maps::$1

[hadoop_total_reduces]
REGEX=TOTAL\_REDUCES\=\"(\d+)\"
FORMAT=job_total_reduces::$1

[hadoop_job_priority]
REGEX=JOB\_PRIORITY\=\"([^\"]+)\"
FORMAT=job_priority::$2

[hadoop_submit_time]
REGEX=SUBMIT\_TIME\=\"([^\"]+)\"
FORMAT=job_submit_time::$1

[hadoop_launch_time]
REGEX=LAUNCH\_TIME\=\"([^\"]+)\"
FORMAT=job_launch_time::$1

[hadoop_job_conf]
REGEX=JOBCONF\=\"([^\"]+)\"
FORMAT=job_conf::$1

[hadoop_view_job]
REGEX=VIEW\_JOB\=\"([^\"]+)\"
FORMAT=job_view_permission::$1

[hadoop_modify_job]
REGEX=MODIFY\_JOB\=\"([^\"]+)\"
FORMAT=job_modify_permission::$1

[hadoop_job_queue]
REGEX=JOB\_QUEUE\=\"([^\"]+)\"
FORMAT=job_queue::$1

[hadoop_job_finish]
REGEX=\sFINISH\_TIME\=\"([^\"]+)
FORMAT=job_finish_time::$1

[hadoop_job_status]
REGEX=\sJOB\_STATUS\=\"([^\"]+)
FORMAT=job_status::$1

[hadoop_finished_maps]
REGEX=\sFINISHED\_MAPS\=\"([^\"]+)
FORMAT=job_finished_maps::$1

[hadoop_finished_reduces]
REGEX=\sFINISHED\_REDUCES\=\"([^\"]+)
FORMAT=job_finished_reduces::$1

[hadoop_failed_maps]
REGEX=\sFAILED\_MAPS\=\"([^\"]+)
FORMAT=job_failed_maps::$1

[hadoop_failed_reduces]
REGEX=\sFAILED\_REDUCES\=\"([^\"]+)
FORMAT=job_failed_reduces::$1

[hadoop_map_counters]
REGEX=MAP\_COUNTERS\=\"([^\"]+)
FORMAT=map_counters::$1

[hadoop_map_counter_internal]
REGEX=\[\(([^\)]+)\)\(([^\)]+)\)(?:(?:\ssnapshot)?\)?)?\(([^\)]+)\)\]\}?
FORMAT=$1::$3
SOURCE_KEY=map_counters
REPEAT_MATCH=True

[hadoop_reduce_counters]
REGEX=REDUCE\_COUNTERS\=\"([^\"]+)
FORMAT=reduce_counters::$1

[hadoop_reduce_counter_internal]
REGEX=\[\(([^\)]+)\)\(([^\)]+)\)(?:(?:\ssnapshot)?\)?)?\(([^\)]+)\)\]\}?
FORMAT=$1::$3
SOURCE_KEY=reduce_counters
REPEAT_MATCH=True

[hadoop_total_counters]
REGEX=\sCOUNTERS\=\"([^\"]+)
FORMAT=total_counters::$1

[hadoop_total_counter_internal]
REGEX=\[\(([^\)]+)\)\(([^\)]+)\)( snapshot)?\)?\(([^\)]+)\)\]\}?\{?
FORMAT=$1::$4
SOURCE_KEY=total_counters
REPEAT_MATCH=True

[hadoop_extract_masters]
REGEX=^([^\.]+)
FORMAT=instance::$1
REPEAT_MATCH=True

[hadoop_extract_slaves]
REGEX=^([^\.]+)
FORMAT=instance::$1
REPEAT_MATCH=True
MV_ADD=True

[hadoop_service_type]
SOURCE_KEY=ARGS
REGEX=-Dproc_?(datanode|tasktracker|jobtracker|secondarynamenode|namenode)
FORMAT=service_type::$1

[fqdn]
REGEX=(?<fqdn>(?:(?:(?:[a-zA-Z0-9][-a-zA-Z0-9]{0,61})?[a-zA-Z0-9])[.])*(?:[a-zA-Z][-a-zA-Z0-9]{0,61}[a-zA-Z0-9]|[a-zA-Z])\.?)

[hadoop_extract_fqdn_ip]
REGEX=\shost\s=\s[[fqdn]]/[[ipv4]]

[hadoop_confdir_from_ps]
REGEX=-classpath_(?<confdir>[^:]+)
FORMAT=confdir::$1

[hadoop_logdir_from_ps]
REGEX=-Dhadoop.log.dir=([^_]+)
FORMAT=logdir::$1

[hadoop_clustername_from_config]
REGEX=(?ms)fs.default.name</name>.*<value>hdfs://([^<]+)
FORMAT=clustername::$1

[bash_user]
SOURCE_KEY=source
REGEX=^\/home\/([^\/]+)\/
FORMAT=user_name::$1

[bash_user_root]
SOURCE_KEY=source
REGEX=^\/(root)\/
FORMAT=user_name::$1
