[__generate_summary_job_eventtypes]
dispatch.earliest_time = -2m
dispatch.latest_time = -1m
search = `hadoop_daemon_logs` job_id=* NOT eventtype=hadoop_deleting_job_userlog_path NOT eventtype=hadoop_adding_job_for_userlog_deletion | bin span=1m _time | rename eventtype as et | stats count by job_id et _time
enableSched = 1
cron_schedule = */1 * * * *
action.summary_index = 1
action.summary_index.marker = hadoop_job_eventtypes

[__generate_lookup_hadoop_jobs]
dispatch.earliest_time = -30d
dispatch.latest_time = now
search = `hadoop_job_history` job_id=* | stats first(job_status) as job_status first(job_name) as job_name first(job_submit_time) as start_time first(job_finish_time) as finish_time first(job_finished_maps) as finished_maps first(job_finished_reduces) as finished_reduces by job_id | outputlookup hadoop_jobs
dispatch.max_count = 100000

[__append_lookup_hadoop_jobs]
dispatch.earliest_time = -30d
dispatch.latest_time = now
search = `hadoop_job_history` job_id=* | append [inputlookup hadoop_job_lookup]| stats first(job_status) as job_status first(job_name) as job_name first(job_submit_time) as start_time first(job_finish_time) as finish_time first(job_finished_maps) as finished_maps first(job_finished_reduces) as finished_reduces by job_id | outputlookup hadoop_jobs
enableSched = 1
cron_schedule = */5 * * * *
dispatch.max_count = 100000

[__retrieve_live_components]
dispatch.earliest_time = -2m
dispatch.latest_time = now
search = `hadoop_ps` | kv hadoop_service_type  | rename service_type as service | dedup host service | table host service
enableSched = 0

[__append_lookup_hadoop_host2hdfs]
cron_schedule = */15 * * * *
dispatch.earliest_time = -15m
dispatch.latest_time = now
enableSched = 1
search = `hadoop_host_hdfs_instance` | append [ inputlookup hadoop_host2hdfs ] | stats count by host hdfs_instance | outputlookup hadoop_host2hdfs

[__append_lookup_hadoop_host2mapred]
cron_schedule = */15 * * * *
dispatch.earliest_time = -15m
dispatch.latest_time = now
enableSched = 1
search = `hadoop_host_mapred_instance` | append [ inputlookup hadoop_host2mapred ] | stats count by host mapred_instance | outputlookup hadoop_host2mapred

[__append_lookup_hadoop_host2rack_logbased]
cron_schedule = */15 * * * *
dispatch.earliest_time = -30m
dispatch.latest_time = now
enableSched = 0
search = `hadoop_jobtracker_logs` "node:" | dedup remote_host | rename remote_host as host remote_host_fqdn as host_fqdn | append [inputlookup hadoop_host2rack] | stats count by host rack_name host_fqdn | fields - count | outputlookup hadoop_host2rack

[__append_lookup_hadoop_host2rack]
cron_schedule = */15 * * * *
dispatch.earliest_time = -30m
dispatch.latest_time = now
enableSched = 1
search = | rest `hadoop_components_rest_uri` | search monitored=1 | hdplookup script=`hadoop_topology_script` | eval host_fqdn=host | append [inputlookup hadoop_host2rack] | dedup host| table host, rack_name, host_fqdn |outputlookup hadoop_host2rack

# if the number of cores is changed within 5m we will not get the value right
# but it should be fine in another 30 minutes - kiru
#
[__append_lookup_hadoop_host2maxcpu]
cron_schedule = */30 * * * *
dispatch.earliest_time = -5m
dispatch.latest_time = now
enableSched = 1
search = `hadoop_max_cpu_host` | append [inputlookup hadoop_host2maxcpu]|dedup host|outputlookup hadoop_host2maxcpu

[__generate_lookup_hadoop_host2rack_logbased]
cron_schedule = 10 0 * * *
dispatch.earliest_time = -30d
dispatch.latest_time = now
enableSched = 0
search = `hadoop_jobtracker_logs` "node:" | dedup remote_host | rename remote_host as host remote_host_fqdn as host_fqdn | stats count by host rack_name host_fqdn | fields - count | outputlookup hadoop_host2rack

[__generate_lookup_hadoop_host2rack]
cron_schedule = 10 0 * * *
dispatch.earliest_time = -30d
dispatch.latest_time = now
enableSched = 1
search = | rest `hadoop_components_rest_uri` | search monitored=1 | hdplookup script=`hadoop_topology_script` | eval host_fqdn=host | dedup host |table host, rack_name, host_fqdn |outputlookup hadoop_host2rack

[__generate_lookup_hadoop_host2hdfs]
cron_schedule = 5 0 * * *
dispatch.earliest_time = -720d
dispatch.latest_time = now
enableSched = 1
search = `hadoop_host_hdfs_instance` | outputlookup hadoop_host2hdfs

[__generate_lookup_hadoop_host2mapred]
cron_schedule = 5 0 * * *
dispatch.earliest_time = -720d@d
dispatch.latest_time = now
enableSched = 1
search = `hadoop_host_mapred_instance` | outputlookup hadoop_host2mapred

[__generate_lookup_hadoop_host2maxcpu]
cron_schedule = 20 0 * * *
dispatch.earliest_time = -5m
dispatch.latest_time = now
enableSched = 1
search = `hadoop_max_cpu_host` | outputlookup hadoop_host2maxcpu
