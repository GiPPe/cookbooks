[noop]
definition = cowsshouldnteatcheesethatwouldmakethemsortofcannibalsinawaydontyouthink | head 1
iseval = 0

[hadoop_conf]
definition = index=hadoopmon_configs
iseval = 0

[hadoop_os]
definition = index=hadoopmon_os
iseval = 0

[hadoop_daemon_logs]
definition = index=hadoopmon_logs
iseval = 0

[hadoop_ps_namenode]
definition = `hadoop_ps` ARGS=*.NameNode
iseval = 0

[hadoop_job_history]
definition = index=hadoopmon_jobs
iseval = 0

[hadoop_ps]
definition = `hadoop_os` source="ps" (COMMAND=*java* OR COMMAND=jsvc.exec) ARGS=*hadoop* NOT ARGS=*hadoop.util* NOT ARGS=*hadoop.tasklog*
iseval = 0

[hadoop_jobtracker_logs]
definition = index=hadoopmon_logs sourcetype=hadoop_jobtracker
iseval = 0

[hadoop_tasktracker_logs]
definition = index=hadoopmon_logs sourcetype=hadoop_tasktracker
iseval = 0

[hadoop_job_mapFinishTime]
definition = if(firstReduceTaskLaunchTime > 0 ,firstReduceTaskLaunchTime, firstJobCleanupTaskLaunchTime)
iseval = 0

[hadoop_metrics]
definition = index=hadoopmon_metrics
iseval = 0

[hadoop_cpu_utilization(1)]
args = CLUSTER_NAME
definition = `hadoop_cpu` clustername=$CLUSTER_NAME$ | multikv FIELDS pctIdle filter all  |  eval cpus=linecount-2 | eval node_cpu_utilized=(100 - pctIdle) |  bucket _time span=30s  | stats avg(node_cpu_utilized) as node_cpu_avg max(cpus) as cpus by _time host clustername | stats sum(node_cpu_avg) as total_cpu sum(cpus) as total_cores  by _time clustername | eval cluster_cpu_utilization = total_cpu / total_cores | table _time clustername cluster_cpu_utilization
iseval = 0

[hadoop_cpu_utilization_by_cluster]
definition = `hadoop_cpu` | multikv FIELDS pctIdle filter all  |  eval cpus=linecount-2 | eval node_cpu_utilized=(100 - pctIdle) |  bucket _time span=30s  | stats avg(node_cpu_utilized) as node_cpu_avg max(cpus) as cpus by _time host clustername | stats sum(node_cpu_avg) as total_cpu sum(cpus) as total_cores  by _time clustername | eval cluster_cpu_utilization = total_cpu / total_cores | table _time clustername cluster_cpu_utilization
iseval = 0

[hadoop_cpu]
definition = index=hadoopmon_os source="cpu"
iseval = 0

[hadoop_cores_total(1)]
args = CLUSTER_NAME
definition = `hadoop_cpu` clustername=$CLUSTER_NAME$ | eval cores = linecount - 2 | stats max(cores) as cores  by  host | stats sum(cores) as total_cores
iseval = 0

[hadoop_cores_total_by_cluster]
definition = `hadoop_cpu` |eval cores = linecount - 2 | stats max(cores) as cores  by  host  clustername | stats sum(cores) as total_cores by clustername
iseval = 0

[__hadoop_postproc_calc_highLoad]
definition = eventstats avg(avg_load) as tot_avg_load count(host) as host_count count(eval(avg_load>`hadoop_highLoad_threshold`)) as high_load | eval tot_avg_load=round(exact(tot_avg_load), 2) | eval high_load_perc=high_load/host_count*100
iseval = 0

[__hadoop_postproc_highLoad_count]
definition = search avg_load >= `hadoop_highLoad_threshold` | stats count
iseval = 0

[__hadoop_postproc_highMem_count]
definition = search avg_memUsedPct >= `hadoop_highMemPct_threshold` | stats count
iseval = 0

[__hadoop_postproc_highSpace_count]
definition = search tot_avg_used >= `hadoop_highSpace_threshold` | stats count
iseval = 0

[__hadoop_postproc_tot_avg_memUsedPct]
definition = eventstats avg(avg_memUsedPct) as tot_avg_memUsedPct | eval tot_avg_memUsedPct=round(tot_avg_memUsedPct, 0)
iseval = 0

[__hadoop_postproc_tot_avg_spaceUsedPct]
definition = stats sum(u) as u sum(t) as t sum(r) as r | eval percentage_used=round((u)/t*100, 0) | gauge percentage_used
iseval = 0

[_hadoop_postproc_highMem_count]
definition = search avg_memUsedPct >= `hadoop_highMemPct_threshold` | stats count
iseval = 0

[__hadoop_postproc_slots_used]
definition = eventstats sum(occupied_map_slots) as total_oms sum(occupied_reduce_slots) as total_ors sum(total_map_slots) as total_ms sum(total_reduce_slots) as total_rs | eval percent_used=round((total_oms + total_ors)/(total_ms + total_rs)*100, 0) | gauge percent_used 
iseval = 0

[__hadoop_postproc_slots_used_host_threshold]
definition = eval percent_used=round((occupied_map_slots+occupied_reduce_slots)/(total_map_slots+total_reduce_slots)*100,0) | search percent_used >= `hadoop_highSlot_threshold` | stats count | rangemap field=count low=0-0 severe=1-10000 default=low 
iseval = 0

[__hadoop_postproc_hdfs_space_remaining]
definition = stats sum(r) as r | rangemap field=r low=`hadoop_hdfs_remaining_low_threshold` elevated=`hadoop_hdfs_remaining_elevated_threshold` severe=`hadoop_hdfs_remaining_severe_threshold` default=low
iseval = 0

[hadoop_avg_load_by_host]
definition = `hadoop_vmstat_logs` | multikv fields loadAvg1mi | stats avg(loadAvg1mi) as avg_load by host | lookup hadoop_host2maxcpu host as host OUTPUT maxcpu as maxcpu | eval avg_load = avg_load/maxcpu
iseval = 0

[hadoop_avg_memUsedPct_by_host]
definition = `hadoop_vmstat_logs` | multikv fields memUsedPct | stats avg(memUsedPct) as avg_memUsedPct by host
iseval = 0

[hadoop_avg_spaceUsedPct_by_namenode]
definition = `hadoop_metrics` ("dfs.FSNamesystem" OR ( context="dfs" sub_context="FSNamesystem")) CapacityRemainingGB=* CapacityTotalGB=* | head 10 | stats first(CapacityUsedGB) as u first(CapacityRemainingGB) as r first(CapacityTotalGB) as t by hostName
iseval = 0

[hadoop_df_logs]
definition = `hadoop_os` sourcetype="df"
iseval = 0

[hadoop_hdfs_home]
definition = MountedOn="/home"
iseval = 0

[hadoop_highLoad_threshold]
definition = .9
iseval = 0

[hadoop_highMemPct_threshold]
definition = 90
iseval = 0

[hadoop_highSpace_threshold]
definition = 90
iseval = 0

[hadoop_highSlot_threshold]
definition = 90
iseval = 0

[hadoop_hdfs_remaining_low_threshold]
definition = 501-100000000
iseval = 0

[hadoop_hdfs_remaining_elevated_threshold]
definition = 200-500
iseval = 0

[hadoop_hdfs_remaining_severe_threshold]
definition = 0-199
iseval = 0

[hadoop_job_summary_table]
definition = index="hadoopmon_jobs" | stats values(job_status) as status by job_id | stats dc(job_id) as total count(eval(like(status, "FAILED"))) as failed count(eval(like(status, "SUCCESS"))) as complete count(eval(like(status, "KILLED"))) as killed count(eval(mvcount(status)==2)) as running count(eval(mvcount(status)==1)) as pending | fields pending running complete failed killed total
iseval = 0

[hadoop_newsFeed_table]
definition = index="hadoopmon_logs" [inputlookup hadoop_news_feed | fields eventtype] | lookup hadoop_news_feed eventtype OUTPUTNEW | eval date=strftime(_time, "%b %d %T") | table date message importance
iseval = 0

[hadoop_slots_used]
definition = `hadoop_metrics` `__hadoop_metrics_mapred_jobtracker` | stats first(occupied_map_slots) as occupied_map_slots first(occupied_reduce_slots) as occupied_reduce_slots first(map_slots) as total_map_slots first(reduce_slots) as total_reduce_slots by host 
iseval = 0

[hadoop_task_summary_table]
definition = `hadoop_job_history` (MapAttempt OR ReduceAttempt) TASK_STATUS | kv auto=T   | stats dc(TASK_ATTEMPT_ID) as attempts last(TASK_STATUS) AS TASK_STATUS by task_id  |  stats count as total  sum(attempts) as attempts count(eval(TASK_STATUS="SUCCESS")) as succeeded  count(eval(TASK_STATUS="FAILED")) as failed | eval "Attempts/Task"=(attempts/total)
iseval = 0

[hadoop_vmstat_logs]
definition = `hadoop_os` sourcetype="vmstat"
iseval = 0

[hadoop_job_mapFinishTime]
definition = if(firstReduceTaskLaunchTime > 0 ,firstReduceTaskLaunchTime, firstJobCleanupTaskLaunchTime)
iseval = 0

[hadoop_jobs_finished(1)]
args = LOOK_BEHIND
definition = `hadoop_jobtracker_logs` JobSummary | table job_id
iseval = 0

[hadoop_jobs_running(1)]
args = LOOK_BEHIND
definition = `hadoop_jobtracker_logs` ("JobInProgress:" OR "JobInProgress$JobSummary:") earliest=$LOOK_BEHIND$| transaction job_id maxevents=-1 | where isnull(status) | table job_id
iseval = 0

[hadoop_jobs_finished]
definition = `hadoop_jobtracker_logs` JobSummary  | table job_id
iseval = 0

[hadoop_job_info(1)]
args = JOB_ID
definition = `hadoop_jobtracker_logs` $JOB_ID$ |  transaction job_id maxevents=-1 startswith=" nMaps" endswith="Deleting localized job conf"  | `__hadoop_jobs_summary_calc_fields`
iseval = 0

[hadoop_jobs_running]
definition = `hadoop_jobtracker_logs` ("JobInProgress:" OR "JobInProgress$JobSummary:") | transaction job_id maxevents=-1 | where isnull(status) | table job_id
iseval = 0

[hadoop_job_summary_table_deprecated]
definition = `hadoop_jobs_summary`  | stats dc(job_id) as total count(eval(like(status, "FAILED"))) as failed count(eval(like(status, "SUCCEEDED"))) as complete count(eval(like(status, "KILLED"))) as killed

[hadoop_job_lookbehind]
definition = -4d
iseval = 0

[__hadoop_jobs_summary_calc_fields]
definition = eval cleanupMsec=(finishTime - firstJobCleanupTaskLaunchTime ) | eval setupMsec = ( firstMapTaskLaunchTime - firstJobSetupTaskLaunchTime) | eval mapMsec=( `hadoop_job_mapFinishTime` - firstMapTaskLaunchTime) | eval reduceMsec = ( firstJobCleanupTaskLaunchTime - `hadoop_job_mapFinishTime` ) | eval jobDurMsec=(finishTime - launchTime) | eval eps=(eventcount*1000/jobDurMsec)
iseval = 0

[hadoop_job_summary_fields]
definition = _time, job_id, attempts, queue, status, user, eps, jobDurMsec, cleanupMsec ,setupMsec , mapMsec, reduceMsec, clusterMapCapacity, clusterReduceCapacity, mapSlotSeconds,reduceSlotsSeconds, numMaps, numReduces
iseval = 0

[hadoop_jobs_summary]
definition = `hadoop_jobtracker_logs` $JobSummary  | dedup job_id | `__hadoop_jobs_summary_calc_fields`
iseval = 0

[hadoop_jobs_summary(1)]
args = JOB_STATUS
definition = `hadoop_jobtracker_logs` $JobSummary  status="$JOB_STATUS$" | dedup job_id | `__hadoop_jobs_summary_calc_fields`
iseval = 0

[hadoop_mr_summary_table]
definition = index=___NOOP___ | stats count |  eval num_maps="-" | eval num_reduces="-" | append [search `hadoop_jobs_summary` | stats sum(numMaps) as num_maps sum(numReduces) as num_reduces ] | stats last(num_maps) as "Map Tasks" last(num_reduces) as "Reduce Tasks"
iseval = 0

[hadoop_components_rest_uri]
definition = /servicesNS/nobody/splunk_for_hadoopops/hadoopops/hadoop_components/ splunk_server=local
iseval = 0

[hadoop_components]
definition = | rest `hadoop_components_rest_uri` | search monitored=1
iseval = 0

[hadoop_health]
definition = `hadoop_os` source="ps" | kv hadoop_service_type | fillnull value=Node service_type | dedup host, service_type | eval marker=-1 | append [`hadoop_components` | rename service as service_type | eval marker = 1] | stats sum(marker) as status_flag by host service_type | search status_flag != -1 OR service_type = "Node" | eval status=if(status_flag==1, "Down" , "Up") | lookup hadoop_host2mapred host OUTPUTNEW | lookup hadoop_host2hdfs host OUTPUTNEW | eval component = case(service_type=="namenode", "hdfs", service_type=="datanode", "hdfs", service_type=="secondarynamenode", "hdfs", service_type="securedatanodestarter", "hdfs", service_type=="jobtracker", "map_reduce", service_type=="tasktracker", "map_reduce") | fillnull value=Node component | eval instance=case(component=="hdfs",hdfs_instance, component=="map_reduce",mapred_instance) | fillnull value=Node instance | table host service_type instance component status 
iseval = 0

[hadoop_health_headlines]
definition = `hadoop_health` | search NOT service_type=Node | stats dc(host) as service_count by instance service_type component status |  eval service = case(service_type=="namenode", "NameNode", service_type=="datanode", "DataNode", service_type=="secondarynamenode", "SecondaryNameNode", service_type=="securedatanodestarter", "SecureDataNodeStarter", service_type=="jobtracker", "JobTracker", service_type=="tasktracker", "TaskTracker") |  eval upcount=if(status != "Down",service_count,0) |  stats sum(service_count) as total_nodes sum(upcount) as up_nodes values(service) as service_summary by service_type instance component  | eval service_health = service_summary . ":" . up_nodes . " / " . total_nodes | stats values(service_health) as service_health by component, instance  | rex field=service_health "(?<service>\w+):(?<health>\d+ / \d+)" max_match=10000 | table instance component service health
iseval = 0

[hadoop_hdfs_config]
definition = `hadoop_conf` source=*core-site.xml  | kv hadoop_conf_xml
iseval = 0

[hadoop_host_hdfs_instance]
definition = `hadoop_hdfs_config`  | stats count last(fs_default_name) as hdfs_instance  by host | rex field=hdfs_instance mode=sed "s,hdfs://,," | table host hdfs_instance
iseval = 0

[hadoop_mapred_config]
definition = `hadoop_conf` source=*mapred-site.xml  | kv hadoop_conf_xml
iseval = 0

[hadoop_host_mapred_instance]
definition = `hadoop_mapred_config`  | stats count last(mapred_job_tracker) as mapred_instance  by host | table host mapred_instance
iseval = 0

[hadoop_ps_hosts_running_hdfs]
definition = `hadoop_ps`

[hadoop_metrics_mapred_utilization]
definition = `hadoop_metrics` `__hadoop_metrics_mapred_jobtracker` | tail 1 | fillnull value=0 waiting_maps waiting_reduces | eval "Maps (Waiting)" = running_maps . " (" .  waiting_maps . ")"  | eval "Reduces (Waiting)" = running_reduces . " (" .  waiting_reduces . ")"   | table "Maps (Waiting)"  "Reduces (Waiting)"
iseval = 0 

[hadoop_metrics_running_jobs_duration]
definition = `hadoop_metrics` (mapred.Queue NOT shuffleOutput NOT tasktracker) OR jobs_running | head 1 | eval running_0=if(isnull(running_0), jobs_running, running_0) | table running_0 running_60 running_300 running_1440 | rename running_0 as " > 0 minutes" running_60 as "> 1 hour" running_300 as "> 5 hours" running_1440 as "> 24 hours"
iseval=0

# multiselect powering
[hadoop_instance_multiselect]
definition = inputlookup hadoop_host2hdfs | rename hdfs_instance as instance | fields instance | append [inputlookup hadoop_host2mapred | rename mapred_instance as instance | fields instance ] | stats count by instance | fields instance

[job_history_multiselect(4)]
args = job_name, job_id, user_name, job_status 
definition = `hadoop_job_history` JOBID $job_id$ | stats values(job_name) as job_name latest(_time) as latest_time latest(job_status) as job_status values(user_name) as user_name values(job_finished_maps) as finished_maps values(job_finished_reduces) as finished_reduces values(job_launch_time) as launch_time values(job_finish_time) as finish_time by job_id | search NOT job_status="RUNNING" AND $job_name$ AND $user_name$ AND $job_status$ | eval duration = (finish_time-launch_time)/1000 | fieldformat launch_time = strftime(launch_time/1000, "%b %d, %T") | fieldformat duration = tostring(duration, "duration") | fields job_name job_id job_status user_name finished_maps finished_reduces launch_time duration | fields - _raw | sort - launch_time
iseval = 0

[jobs_running_multiselect(2)]
args = job_id, user_name
definition = (`hadoop_jobtracker_logs`) OR (`hadoop_job_history`) job_id=* `hadoop_activities_eventtype_exclusions` | stats values(eventtype) as etlist earliest(maps) as total_maps earliest(reduces) as total_reduces count(eval(eventtype="hadoop_task_completed_successfully" AND task_type="m")) as finished_maps count(eval(eventtype="hadoop_task_completed_successfully" AND task_type="r")) as finished_reduces earliest(_time) as start_time earliest(input_size) as input_size earliest(splits) as splits earliest(wait_factor) as wait_factor earliest(user_name) as user_name count(eval(job_status="FAILED" OR job_status="SUCCESS" OR job_status="KILLED")) as done by job_id | search NOT done>0 NOT etlist = "hadoop_job_completed_successfully" AND $job_id$ AND $user_name$ | fields - etlist - _raw - done | sort -start_time | eval duration = now() - start_time | eval total_maps = if(isnull(total_maps),"?", total_maps) | eval total_reduces = if(isnull(total_reduces),"?",total_reduces) | fieldformat start_time=strftime(start_time, "%m/%d/%Y %T") | fieldformat duration = tostring(duration, "duration") | fields job_id finished_maps total_maps finished_reduces total_reduces input_size splits wait_factor user_name start_time duration
iseval = 0

[hadoop_load_avg_by_host]
definition = `hadoop_vmstat_logs` | multikv fields loadAvg1mi | timechart span=1h max(loadAvg1mi) as "Max Load Avg" avg(loadAvg1mi) as "Min Load Avg" by host 
iseval = 0

[hadoop_avg_memUsed_by_host_timechart]
definition = `hadoop_vmstat_logs` | multikv fields memUsedPct | timechart span=5m avg(memUsedPct) as avg_memUsedPct by host 

[hadoop_hdfs_remaining_timechart]
definition = `hadoop_metrics` `__hadoop_metrics_dfs_fsname` CapacityRemainingGB=* CapacityTotalGB=* | timechart span=4h first(CapacityUsedGB) as "HDFS Used" first(CapacityTotalGB) as "HDFS Total" 
iseval = 0 

[hadoop_avg_slots_used_by_host]
definition = `hadoop_metrics` `__hadoop_metrics_mapred_jobtracker` | timechart span=1h avg(occupied_map_slots) as occupied_map_slots avg(occupied_reduce_slots) as occupied_reduce_slots by host 
iseval = 0

[hadoop_health_by_host(1)]
args = host
definition = `hadoop_ps` earliest=-2m $host$ | kv hadoop_service_type | fillnull value=Node service_type | dedup host, service_type | eval marker=-1 | append [`hadoop_components` | search $host$ | rename service as service_type  | eval marker=1] | stats sum(marker) as status_flag by host service_type | search status_flag != -1 | eval status=if(status_flag==1, "Down" , "Up") | lookup hadoop_host2mapred host OUTPUTNEW | lookup hadoop_host2hdfs host OUTPUTNEW | replace "datanode" with "DataNode (DN):" "tasktracker" with "TaskTracker (TT):" "namenode" with "NameNode (NN):" "jobtracker" with "JobTracker (JT):" "secondarynamenode" with "Secondary NameNode (SN):" "balancer" with "Balancer (BL):" in service_type | search NOT service_type=Node | fields service_type status
iseval = 0

[hadoop_nodes_hdfs_used_and_total_stats]
definition = `hadoop_metrics` `__hadoop_metrics_dfs_fsname`  CapacityRemainingGB=* CapacityTotalGB=* | head 1 | stats first(CapacityUsedGB) as hdfs_used first(CapacityTotalGB) as hdfs_total first(CapacityRemainingGB) as hdfs_remaining | eval hdfs_avail_perc=round(hdfs_remaining/hdfs_total, 2)*100 | strcat hdfs_used " GB / " hdfs_total " GB = " hdfs_remaining " GB (" hdfs_avail_perc "%) available" t | fields t | rename t as "Storage (Used/Total): " 
iseval = 0

[hadoop_datanodes_up_down_count]
definition = `hadoop_health` | search service_type="datanode" | stats count(eval(status="Up")) as up count(eval(status="Down")) as down | strcat up "/" down t | fields t | rename t as "Datanodes (Live/Dead): "
iseval = 0

[hadoop_tasktrackers_up_down_count]
definition = `hadoop_health` | search service_type="tasktracker" | stats count(eval(status="Up")) as up count(eval(status="Down")) as down | strcat up "/" down t | fields t | rename t as "Tasktrackers (Live/Dead): "
iseval = 0
 
[hadoop_nodes_status]
definition = `hadoop_health` | `hadoop_lookup_rackname` | rename service_type as services rack_name as rack | replace "Up" with 1 "New" with 1 "Down" with 0 in status | table host services rack status
iseval = 0

[hadoop_heatmap_cpu]
definition = `hadoop_os` source="cpu" CPU="all" | eval heatmap = (100-pctIdle) | stats first(heatmap) as heatmap by host
iseval = 0

[hadoop_heatmap_mem]
definition = `hadoop_os` source=vmstat | stats avg(memUsedPct) as heatmap by host | eval heatmap = round(heatmap, 2)
iseval = 0

[hadoop_heatmap_disk]
definition = `hadoop_os` source=df | convert memk(Avail) as avail memk(Used) as used | stats first(avail) as avail first(used) as used by host Filesystem | stats sum(avail) as avail sum(used) as used by host | eval heatmap = round(used/avail*100, 2) | fields host heatmap
iseval = 0

[hadoop_heatmap_io]
definition = `hadoop_os` source=iostat | stats first(rReq_PS) as reads first(wReq_PS) as writes by host | eval heatmap = reads + writes | fields host heatmap
iseval = 0

[hadoop_nodes_up_down_count]
definition = `hadoop_health` | search NOT service_type="namenode" NOT service_type="jobtracker"  | stats dc(host) as service_count by instance service_type component status 
iseval = 0

[hadoop_jobs_running_waiting_count]
definition = `hadoop_job_history` Job JOBID JOB_STATUS | stats latest(job_status) as job_status by job_id | stats count(eval(job_status="WAITING")) as waiting count(eval(job_status="RUNNING")) as running | strcat running "/" waiting t | fields t | rename t as "Jobs (Running/Waiting): "
iseval = 0

[hadoop_get_rack_from_host(1)]
args = host
definition = rest `hadoop_components_rest_uri` | search monitored=1 $host$ | `hadoop_lookup_rackname` | eval name="Rack: "  | stats count by name rack_name | fields - count
iseval = 0

[hadoop_lookup_rackname]
definition = lookup hadoop_host2rack host OUTPUT rack_name as rack_name1| lookup hadoop_host2rack host_fqdn as host OUTPUT rack_name as rack_name2 | eval rack_name = if ( isnull(rack_name1), rack_name2, rack_name1) | fillnull rack_name value=`hadoop_default_rackname`
iseval = 0

[hadoop_cpu_sparkline_1h(1)]
args = host
definition = `hadoop_os` $host$ source="cpu" CPU="all" | append [stats count | eval _raw="no results" ] | eval used = 100 - pctIdle  | eval name = "CPU:"  | stats  first(name) as name avg(used) as used sparkline(avg(used), 1m) as sl  | eval  used = round(used, 0) . "%" | fillnull used value="unknown - is cpu.sh enabled?"
iseval = 0

[hadoop_mem_sparkline_1h(1)]
args = host
definition = `hadoop_os` $host$ source="vmstat" | multikv fields memUsedPct | append [stats count | eval _raw="no results"] | eval name = "Memory:" | stats first(name) as name avg(memUsedPct) as used sparkline(avg(memUsedPct), 1m) as sl | eval used = round(used, 0) . "%" | fillnull used value="unknown - is vmstat.sh enabled?"
iseval = 0

[hadoop_disk_sparkline_1h(1)]
args = host
definition = `hadoop_os` $host$ source="df" | append [stats count | eval _raw="no results" | eval _time=now()] | eval avail = case(substr(Avail, -1) == "G", tonumber(substr(Avail, 1, len(Avail) - 1)) * 1024, substr(Avail, -1) == "T", tonumber(substr(Avail, 1, len(Avail) - 1)) * 1024 * 1024, substr(Avail, -1) == "M", tonumber(substr(Avail, 1, len(Avail) - 1))) | eval used = case(substr(Used, -1) == "G", tonumber(substr(Used, 1, len(Used) - 1)) * 1024, substr(Used, -1) == "T", tonumber(substr(Used, 1, len(Used) - 1)) * 1024 * 1024, substr(Used, -1) == "M", tonumber(substr(Used, 1, len(Used) - 1))) | fillnull used value=0 | fillnull avail value=0 | fillnull Filesystem value="no results" | bucket span=5m _time | stats avg(used) as used avg(avail) as avail by Filesystem _time | stats avg(used) as used avg(avail) as avail by _time | eval used_perc_no_unit = round(used/(used+avail)*100, 0) | eval used_perc = used_perc_no_unit . "%" | eval name = "Disk:" | stats first(name) as name first(used_perc) as used sparkline(avg(used_perc_no_unit), 5m) as sl | fillnull used value="--%" | fillnull sl value="unknown - is df.sh enabled?"
iseval = 0

[hadoop_cpu_cores(1)]
args = host
definition = `hadoop_os` $host$ source=cpu NOT cpu="all" | append [stats count | eval _raw="no results"] | head 100 | eval name="CPU Cores:" | stats first(name) as name max(CPU) as cpus | eval cpus = cpus + 1 | fillnull cpus value="unknown - is cpu.sh enabled?" 
iseval = 0

[hadoop_mem_mb(1)]
args = host
definition = `hadoop_os` $host$ source=vmstat | append [stats count | eval _raw="no results"] | head 10 | eval name = "Memory:" | stats first(name) as name first(TotalMBytes) as mem | fillnull mem value="unknown - is vmstat.sh enabled?" | eval mem = if(isnum(mem), mem . " MB", mem)
iseval = 0

[hadoop_node_details_mapred(1)]
args = host
definition = rest `hadoop_components_rest_uri` | search monitored=1 service=tasktracker $host$
iseval = 0

[hadoop_node_details_hdfs(1)]
args = host
definition = rest `hadoop_components_rest_uri` | search monitored=1 service=datanode $host$
iseval = 0

[hadoop_slot_capacity_sparkline_1h(1)]
args = host
definition = `hadoop_metrics` $host$ `__hadoop_metrics_mapred_trackers` | append [stats count | eval _raw="no results" | eval _time=now()] | eval ms = if(isnull(mapTaskSlots), map_slots, mapTaskSlots) | eval rs = if(isnull(reduceTaskSlots), reduce_slots, reduceTaskSlots) | fillnull ms rs value=0 | eval total=ms . " / " . rs | eval total_no_slash = ms + rs | eval name = "Capacity M / R slots:" | stats first(name) as name first(total) as total sparkline(avg(total_no_slash), 1m) as sl 
iseval = 0

[hadoop_slot_occupied_sparkline_1h(1)]
args = host
definition = `hadoop_metrics` $host$ `__hadoop_metrics_mapred_trackers` | append [stats count | eval _raw="no results" | eval _time=now()] | eval oms = if(isnull(maps_running), occupied_map_slots, maps_running) | eval ors = if(isnull(reduces_running), occupied_reduce_slots, reduces_running) | fillnull oms ors value=0 | eval total=oms . " / " . ors | eval total_no_slash = oms + ors | eval name = "Occupied M / R slots:" | stats first(name) as name first(total) as total sparkline(avg(total_no_slash), 1m) as sl 
iseval = 0

[hadoop_disk_gb(1)]
args = host
definition = `hadoop_os` $host$ source="df" | append [stats count | eval _raw="no results"] | eval avail = case(substr(Avail, -1) == "G", tonumber(substr(Avail, 1, len(Avail) - 1)) * 1024, substr(Avail, -1) == "T", tonumber(substr(Avail, 1, len(Avail) - 1)) * 1024 * 1024, substr(Avail, -1) == "M", tonumber(substr(Avail, 1, len(Avail) - 1))) | eval used = case(substr(Used, -1) == "G", tonumber(substr(Used, 1, len(Used) - 1)) * 1024, substr(Used, -1) == "T", tonumber(substr(Used, 1, len(Used) - 1)) * 1024 * 1024, substr(Used, -1) == "M", tonumber(substr(Used, 1, len(Used) - 1))) | fillnull used value=0 | fillnull avail value=0 | fillnull Filesystem value="no results" | stats avg(used) as used avg(avail) as avail by Filesystem | eval cap = used + avail | eval name = "Disk:" | stats first(name) as name sum(cap) as cap | eval cap=if(cap=0, "no results", round(cap/1024,0) . " GB")
iseval = 0

[hadoop_total_blocks(1)]
args = host
definition = `hadoop_daemon_logs` $host$ sourcetype="hadoop_datanode" eventtype="hadoop_block_report_results" | append [stats count | eval _raw="no results" ] | head 5 | fillnull total_blocks value=0 | eval name = "Total Blocks:" | stats first(name) as name first(total_blocks) as blocks
iseval = 0

[hadoop_blocks_verified(1)]
args = host
definition = `hadoop_metrics` $host$ "dfs." blocks_verified=* | append [stats count | eval _raw="no results"] | head 3 | fillnull blocks_verified value=0 | eval name = "Verified Blocks:" | stats first(name) as name first(blocks_verified) as blocks_verified
iseval = 0

[__hadoop_metrics_mapred_jobtracker]
definition = ("mapred.jobtracker" OR ( context="mapred" sub_context="jobtracker"))
iseval = 0

[__hadoop_metrics_mapred_trackers]
definition = ("mapred.tasktracker"  OR "mapred.jobtracker" OR ( context="mapred" (sub_context="tasktracker" OR sub_context="jobtracker")))
iseval = 0

[__hadoop_metrics_dfs_fsname]
definition = ("dfs.FSNamesystem" OR ( context="dfs" sub_context="FSNamesystem"))
iseval = 0

[hadoop_default_rackname]
definition = default-rack
iseval = 0

[hadoop_summary]
definition = index=summary
iseval = 0

[hadoop_job_eventtype_summary(1)]
args = job_id
definition = `hadoop_summary` marker=hadoop_job_eventtypes $job_id$ `hadoop_job_eventtype_summary_exclusions` [search `hadoop_summary` marker=hadoop_job_eventtypes $job_id$ `hadoop_job_eventtype_summary_exclusions` |  stats min(info_min_time) as earliest max(info_max_time) as latest | eval earliest=earliest-60 | eval latest=latest+60 | fields earliest latest | format "(" "(" "" ")" "OR" ")" ]
iseval = 0

[hadoop_job_eventtype_summary_exclusions]
definition = NOT et=hadoop_adding_job_for_userlog_deletion NOT et=hadoop_deleting_job_userlog_path NOT et=hadoop_client_trace NOT et=hadoop_job_conf_deleting NOT et=hadoop_job_retired NOT et=hadoop_task_output_size
iseval = 0

[hadoop_activities_eventtype_exclusions]
definition = NOT eventtype=hadoop_job_conf_deleting NOT eventtype=hadoop_job_retired NOT eventtype=hadoop_job_history_file_loading
iseval = 0

[hadoop_topology_script]
definition = ./topology.py
iseval = 0

[__hadoop_postproc_hdfs_capacity]
definition = eval capacity=u+"/"+t+" GB" | fields capacity
iseval = 0

[__hadoop_postproc_slots_capacity]
definition = eval total_slots=total_map_slots+total_reduce_slots |eval used_slots=occupied_map_slots+occupied_reduce_slots| eval capacity=used_slots+"/"+total_slots| fields capacity

[hadoop_max_cpu_host]
definition = index=hadoopmon_os source="cpu" | stats max(CPU) as maxcpu by host | eval maxcpu = maxcpu + 1
iseval = 0
